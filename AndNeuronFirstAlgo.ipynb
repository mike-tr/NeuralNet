{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit ('cv': conda)",
   "metadata": {
    "interpreter": {
     "hash": "b47b01fbeacb6918fe178353ac26b8951e929afac8e10f32a05608fcfdb3257b"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## How the training Works\n",
    "\n",
    "* consider there is an input X, that should yield a positive answer 1.\n",
    "* we want to tweak the weights w, such that w * x > 0\n",
    "* we add the bias to x therefore x(n+1) = 1\n",
    "\n",
    "### Notice: w is the same shape as x.\n",
    "\n",
    "* consider swapping w to x ->  it will result in x*x \n",
    "* notice that x*x is a dot product and therefore be a positive number,\n",
    "* x\\*x = x(1 to n)\\*x(1 to n) + x(n+1)\\*x(n+1) >= x(n+1)\\*x(n+1) = 1\n",
    "\n",
    "hence if w = x on the next test it will score > 0 given the same x.\n",
    "\n",
    "### We want to keep the changes\n",
    "* we do not wish to completly override w.\n",
    "* therefore we say w = w + X\n",
    "\n",
    "notice that by the same reasoning w(new)\\*x > w(old)\\*x.\n",
    "\n",
    "\n",
    "### Negative exmpales\n",
    "\n",
    "* on the other hand consider X and y being negative,\n",
    "* hence we want w * X < 0\n",
    "\n",
    "####  Notice : \n",
    "* therefore consider -x * x would be a negative number.\n",
    "* hence if we set w = w - x => w(new)\\*x < w(old)\\*x\n",
    "\n",
    "thuse there is a better chance we will get a score less then 0 next try.\n",
    "\n",
    "\n",
    "### Normalizing X\n",
    "* another issue would be that not all inputs are equal\n",
    "* the input \\[1, 1\\] will have 10% of the effect of the input \\[10, 10\\] on the weights.\n",
    "* therefore we normalize, a.k.a we want the absolute sum of all cells in input to be exactly 1.\n",
    "\n",
    "\n",
    "### Normalizing effect\n",
    "* again notice that x.normalzie() * x > 0\n",
    "* hence if we say w = w + x.normalized we will score better.\n",
    "\n",
    "## Final algorithm :\n",
    "\n",
    ">if y = 1 do:\n",
    "\n",
    ">>   w = w + x.normalzied()\n",
    "\n",
    "> else \n",
    "\n",
    ">>    w = w - x.normalzied()\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from neuron import Neuron"
   ]
  },
  {
   "source": [
    "## Test on AND "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 1, 1],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 1, 0],\n",
       "       [1, 1, 0, 1],\n",
       "       [1, 1, 1, 1]])"
      ]
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "# A AND B or B and C\n",
    "data = np.array([\n",
    "    # A  B  C  O\n",
    "    [0, 0, 0, 0],\n",
    "    [0, 0, 1, 0],\n",
    "    [0, 1, 0, 0],\n",
    "    [0, 1, 1, 1],\n",
    "    [1, 0, 0, 0],\n",
    "    [1, 0, 1, 0],\n",
    "    [1, 1, 0, 1],\n",
    "    [1, 1, 1, 1],\n",
    "])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0 0 0]\n [0 0 1]\n [0 1 0]\n [0 1 1]\n [1 0 0]\n [1 0 1]\n [1 1 0]\n [1 1 1]]\n[[0]\n [0]\n [0]\n [1]\n [0]\n [0]\n [1]\n [1]]\n"
     ]
    }
   ],
   "source": [
    "x = data[:, :-1]\n",
    "y = data[:, -1:]\n",
    "\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "source": [
    "## Add bias to input"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1.],\n",
       "       [0., 0., 1., 1.],\n",
       "       [0., 1., 0., 1.],\n",
       "       [0., 1., 1., 1.],\n",
       "       [1., 0., 0., 1.],\n",
       "       [1., 0., 1., 1.],\n",
       "       [1., 1., 0., 1.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "source": [
    "x_bias = np.ones((x.shape[0], 1))\n",
    "x_train = np.c_[x, x_bias]\n",
    "x_train"
   ]
  },
  {
   "source": [
    "## Create neuron and train\n",
    "* we train by looping over all examples untill the neuron do not fails or we run more then 100 times."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "metadata": {},
     "execution_count": 82
    }
   ],
   "source": [
    "n = Neuron(x_train, y)\n",
    "n.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n-------- run 0 --------\nprediction : 0 , actual : 0\nprediction : 0 , actual : 0\nprediction : 0 , actual : 0\nprediction : 0 , actual : 1\nprediction : 1 , actual : 0\nprediction : 0 , actual : 0\nprediction : 0 , actual : 1\nprediction : 1 , actual : 1\nEnd of run Num of Errors : 3\nweights : \n[[-0.16666667]\n [ 0.66666667]\n [ 0.33333333]\n [ 0.16666667]]\n\n-------- run 1 --------\nprediction : 1 , actual : 0\nprediction : 0 , actual : 0\nprediction : 0 , actual : 0\nprediction : 1 , actual : 1\nprediction : 0 , actual : 0\nprediction : 0 , actual : 0\nprediction : 0 , actual : 1\nprediction : 1 , actual : 1\nEnd of run Num of Errors : 2\nweights : \n[[ 0.16666667]\n [ 1.        ]\n [ 0.33333333]\n [-0.5       ]]\n\n-------- run 2 --------\nprediction : 0 , actual : 0\nprediction : 0 , actual : 0\nprediction : 1 , actual : 0\nprediction : 0 , actual : 1\nprediction : 0 , actual : 0\nprediction : 1 , actual : 0\nprediction : 0 , actual : 1\nprediction : 1 , actual : 1\nEnd of run Num of Errors : 4\nweights : \n[[ 0.16666667]\n [ 1.16666667]\n [ 0.33333333]\n [-0.66666667]]\n\n-------- run 3 --------\nprediction : 0 , actual : 0\nprediction : 0 , actual : 0\nprediction : 1 , actual : 0\nprediction : 0 , actual : 1\nprediction : 0 , actual : 0\nprediction : 0 , actual : 0\nprediction : 1 , actual : 1\nprediction : 1 , actual : 1\nEnd of run Num of Errors : 2\nweights : \n[[ 0.16666667]\n [ 1.        ]\n [ 0.66666667]\n [-0.83333333]]\n\n-------- run 4 --------\nprediction : 0 , actual : 0\nprediction : 0 , actual : 0\nprediction : 1 , actual : 0\nprediction : 0 , actual : 1\nprediction : 0 , actual : 0\nprediction : 1 , actual : 0\nprediction : 0 , actual : 1\nprediction : 1 , actual : 1\nEnd of run Num of Errors : 4\nweights : \n[[ 0.16666667]\n [ 1.16666667]\n [ 0.66666667]\n [-1.        ]]\n\n-------- run 5 --------\nprediction : 0 , actual : 0\nprediction : 0 , actual : 0\nprediction : 1 , actual : 0\nprediction : 0 , actual : 1\nprediction : 0 , actual : 0\nprediction : 0 , actual : 0\nprediction : 0 , actual : 1\nprediction : 1 , actual : 1\nEnd of run Num of Errors : 3\nweights : \n[[ 0.5       ]\n [ 1.33333333]\n [ 1.        ]\n [-0.83333333]]\n\n-------- run 6 --------\nprediction : 0 , actual : 0\nprediction : 1 , actual : 0\nprediction : 0 , actual : 0\nprediction : 1 , actual : 1\nprediction : 0 , actual : 0\nprediction : 0 , actual : 0\nprediction : 1 , actual : 1\nprediction : 1 , actual : 1\nEnd of run Num of Errors : 1\nweights : \n[[ 0.5       ]\n [ 1.33333333]\n [ 0.5       ]\n [-1.33333333]]\n\n-------- run 7 --------\nprediction : 0 , actual : 0\nprediction : 0 , actual : 0\nprediction : 0 , actual : 0\nprediction : 1 , actual : 1\nprediction : 0 , actual : 0\nprediction : 0 , actual : 0\nprediction : 1 , actual : 1\nprediction : 1 , actual : 1\n"
     ]
    }
   ],
   "source": [
    "n = Neuron(x_train, y)\n",
    "\n",
    "end_run = -1\n",
    "\n",
    "for i in range(100):\n",
    "    print(\"\\n-------- run\", i, \"--------\")\n",
    "    err = 0\n",
    "    done = True\n",
    "    for j in range(x_train.shape[0]):\n",
    "        pred = n.predict(x_train[j])\n",
    "        actual = y[j]\n",
    "        print(\"prediction :\", pred, \", actual :\", actual[0])\n",
    "        change = n.train_one(x_train[j], actual)\n",
    "        if change:\n",
    "            err += 1\n",
    "            done = False\n",
    "    if done:\n",
    "        end_run = i\n",
    "        break\n",
    "    else:\n",
    "        print(\"End of run Num of Errors :\", err)\n",
    "        print(\"weights : \")\n",
    "        print(n.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n------ training done ------\ntraining done on run : 7\nfinal weights :\n[[ 0.5       ]\n [ 1.33333333]\n [ 0.5       ]\n [-1.33333333]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n------ training done ------\")\n",
    "print(\"training done on run :\", end_run)\n",
    "print(\"final weights :\")\n",
    "print(n.weights)"
   ]
  }
 ]
}